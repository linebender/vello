// See https://research.nvidia.com/sites/default/files/pubs/2016-03_Single-pass-Parallel-Prefix/nvr-2016-002.pdf

#version 450
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_KHR_shader_subgroup_ballot : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_memory_scope_semantics : enable

layout(local_size_x = 1024) in;

// One workgroup processes workgroup size * N_ROWS elements.
#define N_ROWS 16

layout(set = 0, binding = 0) readonly buffer InBuf {
    uint[] in_buf;
};

layout(set = 0, binding = 1) buffer OutBuf {
    uint[] out_buf;
};

// work_buf[0] is the tile id
// work_buf[i * 4 + 1] is the flag for tile i
// work_buf[i * 4 + 2] is the aggregate for tile i
// work_buf[i * 4 + 3] is the prefix for tile i
layout(set = 0, binding = 2) buffer WorkBuf {
    uint[] work_buf;
};

// These correspond to X, A, P respectively in the paper.
#define FLAG_NOT_READY 0
#define FLAG_AGGREGATE_READY 1
#define FLAG_PREFIX_READY 2

shared uint shared_tile;
shared uint shared_prefix;
// Note: the subgroup size and other dimensions are hard-coded.
// TODO: make it more adaptive.
shared uint chunks[32];

void main() {
    uint local_ix = gl_LocalInvocationID.x;
    // Determine tile to process by atomic counter (implement idea from
    // section 4.4 in the paper).
    if (local_ix == 0) {
        shared_tile = atomicAdd(work_buf[0], 1);
    }
    barrier();
    uint my_tile = shared_tile;
    uint mem_base = my_tile * 16384;
    uint aggregates[N_ROWS];

    // Interleave reading of data, computing row prefix sums, and aggregate
    // (step 3 of paper).
    uint total = 0;
    for (uint i = 0; i < N_ROWS; i++) {
        uint ix = (local_ix & 0x3e0) * N_ROWS + i * 32 + (local_ix & 0x1f);
        uint data = in_buf[mem_base + ix];
        uint row = subgroupInclusiveAdd(data);
        total += row;
        aggregates[i] = row;
    }
    if (gl_SubgroupInvocationID == 31) {
        chunks[local_ix >> 5] = total;
    }

    barrier();
    if (local_ix < 32) {
        uint chunk = chunks[gl_SubgroupInvocationID];
        total = subgroupInclusiveAdd(chunk);
        chunks[gl_SubgroupInvocationID] = total;
    }

    uint exclusive_prefix = 0;
    if (local_ix == 31) {
        atomicStore(work_buf[my_tile * 4 + 2], total, gl_ScopeDevice, gl_StorageSemanticsBuffer, gl_SemanticsRelaxed);
        uint flag = FLAG_AGGREGATE_READY;
        if (my_tile == 0) {
            atomicStore(work_buf[my_tile * 4 + 3], total, gl_ScopeDevice, gl_StorageSemanticsBuffer, gl_SemanticsRelaxed);
            flag = FLAG_PREFIX_READY;
        }
        atomicStore(work_buf[my_tile * 4 + 1], flag, gl_ScopeDevice, gl_StorageSemanticsBuffer, gl_SemanticsRelease);
        if (my_tile != 0) {
            // step 4: decoupled lookback
            uint look_back_ix = my_tile - 1;
            while (true) {
                flag = atomicLoad(work_buf[look_back_ix * 4 + 1], gl_ScopeDevice, gl_StorageSemanticsBuffer, gl_SemanticsAcquire);
                if (flag == FLAG_PREFIX_READY) {
                    uint their_prefix = atomicLoad(work_buf[look_back_ix * 4 + 3], gl_ScopeDevice, gl_StorageSemanticsBuffer, gl_SemanticsRelaxed);
                    exclusive_prefix = their_prefix + exclusive_prefix;
                    break;
                } else if (flag == FLAG_AGGREGATE_READY) {
                    uint their_agg = atomicLoad(work_buf[look_back_ix * 4 + 2], gl_ScopeDevice, gl_StorageSemanticsBuffer, gl_SemanticsRelaxed);
                    exclusive_prefix = their_agg + exclusive_prefix;
                    look_back_ix--;
                }
                // else spin
            }

            // step 5: compute inclusive prefix
            uint inclusive_prefix = exclusive_prefix + total;
            shared_prefix = exclusive_prefix;
            atomicStore(work_buf[my_tile * 4 + 3], inclusive_prefix, gl_ScopeDevice, gl_StorageSemanticsBuffer, gl_SemanticsRelaxed);
            flag = FLAG_PREFIX_READY;
            atomicStore(work_buf[my_tile * 4 + 1], flag, gl_ScopeDevice, gl_StorageSemanticsBuffer, gl_SemanticsRelease);
        }
    }
    uint prefix = 0;
    barrier();
    if (my_tile != 0) {
        prefix = shared_prefix;
    }

    // step 6: perform partition-wide scan
    if (local_ix >> 5 > 0) {
        prefix += chunks[(local_ix >> 5) - 1];
    }
    for (uint i = 0; i < N_ROWS; i++) {
        uint ix = (local_ix & 0x3e0) * N_ROWS + i * 32 + (local_ix & 0x1f);
        uint agg = aggregates[i];
        out_buf[mem_base + ix] = prefix + agg;
        prefix += subgroupBroadcast(agg, 31);
    }
}
